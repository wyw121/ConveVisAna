"""æµ‹è¯•æ›´æ–°åçš„é…ç½®æ˜¯å¦æ­£å¸¸å·¥ä½œ"""
from config.llm_config import get_api_key, get_model_for_task, LLMConfig
from core.custom_llm import ChatAIAPIModel

print("\n" + "="*60)
print("æµ‹è¯•ç¡…åŸºæµåŠ¨å…è´¹æ¨¡å‹é›†æˆ")
print("="*60)

# 1. è·å–é…ç½®
api_key = get_api_key()
model = get_model_for_task("evaluation")

print(f"\nâœ… API Key: {api_key[:20]}...{api_key[-10:]}")
print(f"âœ… æ¨¡å‹: {model}")
print(f"âœ… Base URL: {LLMConfig.get_base_url()}")

# 2. åˆ›å»º LLM å®ä¾‹
llm = ChatAIAPIModel(api_key=api_key, model=model)
print(f"\nâœ… LLM åˆå§‹åŒ–æˆåŠŸ")

# 3. æµ‹è¯•ç®€å•è°ƒç”¨
print(f"\nğŸ”„ æµ‹è¯• API è°ƒç”¨...")
try:
    response = llm.generate("ä½ å¥½ï¼Œè¯·å›å¤'æµ‹è¯•æˆåŠŸ'")
    print(f"âœ… API è°ƒç”¨æˆåŠŸï¼")
    print(f"ğŸ“ æ¨¡å‹å›å¤: {response[:100]}")
    print("\n" + "="*60)
    print("âœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼é…ç½®æ­£ç¡®ï¼Œå¯ä»¥æ­£å¸¸ä½¿ç”¨")
    print("="*60 + "\n")
except Exception as e:
    print(f"âŒ API è°ƒç”¨å¤±è´¥: {str(e)}")
    print("\nè¯·æ£€æŸ¥:")
    print("1. API Key æ˜¯å¦æ­£ç¡®")
    print("2. ç¡…åŸºæµåŠ¨è´¦æˆ·æ˜¯å¦æœ‰å¯ç”¨é¢åº¦")
    print("3. æ˜¯å¦éœ€è¦å®Œæˆèº«ä»½éªŒè¯")
