"""
LLM é…ç½®ç®¡ç†ä¸­å¿ƒ
ç»Ÿä¸€ç®¡ç†æ‰€æœ‰ LLM API ç›¸å…³é…ç½®
"""
import os
from typing import Optional, Dict
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv(override=True)


class LLMConfig:
    """LLM é…ç½®ç®¡ç†ç±» - å•ä¸€æ•°æ®æº (Single Source of Truth)"""
    
    # ============ API Key é…ç½® ============
    
    @staticmethod
    def get_api_key() -> Optional[str]:
        """
        è·å– API Keyï¼ˆæŒ‰ä¼˜å…ˆçº§ï¼‰
        
        ä¼˜å…ˆçº§:
        1. API_KEY_OVERRIDE (è¿è¡Œæ—¶æ³¨å…¥ï¼Œæœ€é«˜ä¼˜å…ˆçº§)
        2. CHATAIAPI_KEY (ChatAI ä¸»é”®)
        3. CHATAI_API_KEY (ChatAI åˆ«å)
        4. OPENAI_API_KEY (OpenAI åŸç”Ÿ)
        
        Returns:
            API Key å­—ç¬¦ä¸²ï¼Œå¦‚æœæœªé…ç½®åˆ™è¿”å› None
        """
        return (
            os.getenv("API_KEY_OVERRIDE")
            or os.getenv("CHATAIAPI_KEY")
            or os.getenv("CHATAI_API_KEY")
            or os.getenv("OPENAI_API_KEY")
        )
    
    @staticmethod
    def get_base_url() -> str:
        """
        è·å– API åŸºç¡€ URL
        
        Returns:
            API åŸºç¡€ URLï¼Œé»˜è®¤ä¸ºç¡…åŸºæµåŠ¨ API åœ°å€
        """
        return os.getenv(
            "CHATAI_BASE_URL", 
            "https://api.siliconflow.cn/v1"
        )
    
    @staticmethod
    def is_api_available() -> bool:
        """
        æ£€æŸ¥ API Key æ˜¯å¦å·²é…ç½®
        
        Returns:
            True å¦‚æœ API Key å­˜åœ¨ï¼Œå¦åˆ™ False
        """
        return bool(LLMConfig.get_api_key())
    
    # ============ æ¨¡å‹é…ç½® ============
    
    # é»˜è®¤æ¨¡å‹é…ç½® - ç»Ÿä¸€ä½¿ç”¨ Qwen2.5 è·å¾—æœ€ä½³å…¼å®¹æ€§
    DEFAULT_MODELS = {
        "evaluation": "Qwen/Qwen2.5-7B-Instruct",  # è´¨é‡è¯„ä¼°ï¼š100%æµ‹è¯•é€šè¿‡ï¼ŒJSONå®Œç¾
        "flow_analysis": "Qwen/Qwen2.5-7B-Instruct",  # æµç¨‹åˆ†æï¼šç¨³å®šå¯é ï¼Œæ— æ€ç»´é“¾å¹²æ‰°
        "general": "Qwen/Qwen2.5-7B-Instruct"  # é€šç”¨ï¼šæœ€æ–°ç‰ˆæœ¬ï¼Œç»¼åˆèƒ½åŠ›æœ€å¼º
    }
    
    @classmethod
    def get_default_model(cls, task: str = "general") -> str:
        """
        è·å–æŒ‡å®šä»»åŠ¡çš„é»˜è®¤æ¨¡å‹
        
        æ”¯æŒé€šè¿‡ç¯å¢ƒå˜é‡è¦†ç›–ï¼Œä¾‹å¦‚:
        - DEFAULT_MODEL_EVALUATION
        - DEFAULT_MODEL_FLOW_ANALYSIS
        - DEFAULT_MODEL_GENERAL
        
        Args:
            task: ä»»åŠ¡ç±»å‹ (evaluation, flow_analysis, general)
        
        Returns:
            æ¨¡å‹åç§°å­—ç¬¦ä¸²
        """
        # æ”¯æŒç¯å¢ƒå˜é‡è¦†ç›–
        env_key = f"DEFAULT_MODEL_{task.upper()}"
        return os.getenv(env_key, cls.DEFAULT_MODELS.get(task, cls.DEFAULT_MODELS["general"]))
    
    # ============ è¶…æ—¶å’Œé‡è¯•é…ç½® ============
    
    @staticmethod
    def get_timeout() -> tuple:
        """
        è·å–è¶…æ—¶é…ç½® (è¿æ¥è¶…æ—¶, è¯»å–è¶…æ—¶)
        
        æ”¯æŒç¯å¢ƒå˜é‡ CHATAI_TIMEOUTï¼Œæ ¼å¼:
        - "è¿æ¥,è¯»å–" ä¾‹å¦‚ "15,60"
        - å•å€¼ ä¾‹å¦‚ "30" (è¿æ¥å’Œè¯»å–ä½¿ç”¨åŒä¸€å€¼)
        
        Returns:
            å…ƒç»„ (è¿æ¥è¶…æ—¶ç§’æ•°, è¯»å–è¶…æ—¶ç§’æ•°)ï¼Œé»˜è®¤ (15, 60)
        """
        env_timeout = os.getenv("CHATAI_TIMEOUT")
        if env_timeout:
            try:
                parts = [p.strip() for p in env_timeout.split(',')]
                if len(parts) == 2:
                    return (float(parts[0]), float(parts[1]))
                else:
                    t = float(parts[0])
                    return (t, t)
            except Exception:
                pass
        return (15, 60)  # é»˜è®¤å€¼
    
    @staticmethod
    def get_retry_config() -> Dict[str, float]:
        """
        è·å–é‡è¯•é…ç½®
        
        æ”¯æŒç¯å¢ƒå˜é‡:
        - CHATAI_RETRY_TOTAL: é‡è¯•æ€»æ¬¡æ•° (é»˜è®¤ 3)
        - CHATAI_RETRY_BACKOFF: é€€é¿å› å­ (é»˜è®¤ 1.5)
        
        Returns:
            åŒ…å« 'total' å’Œ 'backoff' çš„å­—å…¸
        """
        return {
            "total": int(os.getenv("CHATAI_RETRY_TOTAL", "3")),
            "backoff": float(os.getenv("CHATAI_RETRY_BACKOFF", "1.5"))
        }
    
    # ============ æ¨¡å‹é¢„è®¾å’Œå…ƒæ•°æ® ============
    
    SUPPORTED_MODELS = {
        # æ¨èé¦–é€‰ï¼šç¡…åŸºæµåŠ¨å…è´¹æ¨¡å‹
        "Qwen/Qwen2.5-7B-Instruct": {
            "name": "Qwen 2.5 7B Instruct",
            "provider": "SiliconFlow (ç¡…åŸºæµåŠ¨)",
            "cost": "free",
            "speed": "fast",  # å¹³å‡ 1.88ç§’
            "quality": "excellent",
            "recommended_for": ["evaluation", "flow_analysis", "general", "json_output", "structured_output"],
            "description": "âœ… æ¨èé¦–é€‰ï¼šæœ€æ–°ç‰ˆæœ¬ï¼Œ100%æµ‹è¯•é€šè¿‡ï¼ŒJSONæ ¼å¼å®Œç¾ï¼Œæ— æ€ç»´é“¾å¹²æ‰°",
            "notes": "ç»¼åˆèƒ½åŠ›æœ€å¼ºï¼Œé€‚åˆæ‰€æœ‰ä»»åŠ¡åœºæ™¯",
            "test_results": {
                "success_rate": "100%",
                "avg_response_time": 1.88,
                "json_compatibility": "perfect"
            }
        },
        
        # å¤‡é€‰ï¼šç¡…åŸºæµåŠ¨å…è´¹æ¨¡å‹
        "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B": {
            "name": "DeepSeek R1 Distill Qwen 7B",
            "provider": "SiliconFlow (ç¡…åŸºæµåŠ¨)",
            "cost": "free",
            "speed": "very_fast",  # å¹³å‡ 2.10ç§’
            "quality": "good",
            "recommended_for": ["reasoning", "deep_analysis"],
            "description": "æ¨ç†æ¨¡å‹ï¼Œæ€ç»´èƒ½åŠ›å¼ºä½†è¾“å‡ºåŒ…å«æ€ç»´é“¾",
            "notes": "âš ï¸ è¾“å‡ºæ ¼å¼ä¸ç¨³å®šï¼ŒJSONå…¼å®¹æ€§å·®ï¼Œä¸æ¨èç”¨äºç»“æ„åŒ–è¾“å‡ºä»»åŠ¡",
            "test_results": {
                "success_rate": "50%",
                "avg_response_time": 2.10,
                "json_compatibility": "poor"
            }
        },
        "Qwen/Qwen2-7B-Instruct": {
            "name": "Qwen 2 7B Instruct",
            "provider": "SiliconFlow (ç¡…åŸºæµåŠ¨)",
            "cost": "free",
            "speed": "very_fast",  # å¹³å‡ 1.01ç§’
            "quality": "good",
            "recommended_for": ["general", "json_output"],
            "description": "æ—§ç‰ˆæœ¬ï¼Œé€Ÿåº¦æ›´å¿«ä½†èƒ½åŠ›ç•¥å¼±",
            "notes": "å¤‡é€‰æ–¹æ¡ˆï¼Œå¦‚éœ€æ›´å¿«å“åº”é€Ÿåº¦å¯è€ƒè™‘",
            "test_results": {
                "success_rate": "100%",
                "avg_response_time": 1.01,
                "json_compatibility": "perfect"
            }
        },
        
        # ä»¥ä¸‹æ¨¡å‹å·²ç¦ç”¨ï¼ˆä»…ä½œè®°å½•ï¼‰
        # "claude-3-5-sonnet-20240620": {...},
        # "deepseek-chat": {...},
        # "deepseek-ai/DeepSeek-V3.2": {...},
        # "gpt-4o-mini": {...},
    }
    
    @classmethod
    def get_model_info(cls, model_name: str) -> Optional[Dict]:
        """
        è·å–æ¨¡å‹è¯¦ç»†ä¿¡æ¯
        
        Args:
            model_name: æ¨¡å‹åç§°
        
        Returns:
            åŒ…å«æ¨¡å‹å…ƒæ•°æ®çš„å­—å…¸ï¼Œå¦‚æœæ¨¡å‹ä¸å­˜åœ¨è¿”å› None
        """
        return cls.SUPPORTED_MODELS.get(model_name)
    
    @classmethod
    def list_models(cls, provider: Optional[str] = None, 
                    cost: Optional[str] = None,
                    recommended_for: Optional[str] = None) -> list:
        """
        åˆ—å‡ºæ‰€æœ‰æ”¯æŒçš„æ¨¡å‹ï¼ˆæ”¯æŒè¿‡æ»¤ï¼‰
        
        Args:
            provider: æŒ‰æä¾›å•†è¿‡æ»¤ (ä¾‹å¦‚ "DeepSeek", "Anthropic")
            cost: æŒ‰æˆæœ¬è¿‡æ»¤ (ä¾‹å¦‚ "low", "medium", "high")
            recommended_for: æŒ‰æ¨èç”¨é€”è¿‡æ»¤ (ä¾‹å¦‚ "evaluation", "flow_analysis")
        
        Returns:
            æ¨¡å‹åç§°åˆ—è¡¨
        """
        models = []
        for model_name, info in cls.SUPPORTED_MODELS.items():
            # åº”ç”¨è¿‡æ»¤æ¡ä»¶
            if provider and info.get("provider") != provider:
                continue
            if cost and info.get("cost") != cost:
                continue
            if recommended_for and recommended_for not in info.get("recommended_for", []):
                continue
            models.append(model_name)
        return models
    
    @classmethod
    def get_models_by_cost(cls) -> Dict[str, list]:
        """
        æŒ‰æˆæœ¬åˆ†ç»„è¿”å›æ¨¡å‹
        
        Returns:
            å­—å…¸ï¼Œé”®ä¸ºæˆæœ¬ç­‰çº§ï¼Œå€¼ä¸ºæ¨¡å‹åç§°åˆ—è¡¨
        """
        result = {
            "free": [],
            "very_low": [],
            "low": [],
            "medium": [],
            "high": []
        }
        for model_name, info in cls.SUPPORTED_MODELS.items():
            cost = info.get("cost", "medium")
            if cost in result:
                result[cost].append(model_name)
        return result
    
    @classmethod
    def recommend_model(cls, task: str, priority: str = "balanced") -> str:
        """
        æ™ºèƒ½æ¨èæ¨¡å‹
        
        Args:
            task: ä»»åŠ¡ç±»å‹ (evaluation, flow_analysis, general)
            priority: ä¼˜å…ˆçº§ (cost - ä¼˜å…ˆæˆæœ¬, speed - ä¼˜å…ˆé€Ÿåº¦, quality - ä¼˜å…ˆè´¨é‡, balanced - å¹³è¡¡)
        
        Returns:
            æ¨èçš„æ¨¡å‹åç§°
        """
        # ä¼˜å…ˆä½¿ç”¨ç¯å¢ƒå˜é‡é…ç½®
        env_model = cls.get_default_model(task)
        if env_model != cls.DEFAULT_MODELS.get(task, cls.DEFAULT_MODELS["general"]):
            return env_model
        
        # ç»Ÿä¸€ä½¿ç”¨ç¡…åŸºæµåŠ¨å…è´¹æ¨¡å‹
        return "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B"


# ============ ä¾¿æ·å‡½æ•° ============

def get_api_key() -> Optional[str]:
    """
    å¿«æ·è·å– API Key
    
    Returns:
        API Key å­—ç¬¦ä¸²ï¼Œå¦‚æœæœªé…ç½®åˆ™è¿”å› None
    """
    return LLMConfig.get_api_key()


def get_model_for_task(task: str) -> str:
    """
    å¿«æ·è·å–ä»»åŠ¡é»˜è®¤æ¨¡å‹
    
    Args:
        task: ä»»åŠ¡ç±»å‹ (evaluation, flow_analysis, general)
    
    Returns:
        æ¨¡å‹åç§°å­—ç¬¦ä¸²
    """
    return LLMConfig.get_default_model(task)


def get_base_url() -> str:
    """
    å¿«æ·è·å– API åŸºç¡€ URL
    
    Returns:
        API åŸºç¡€ URL å­—ç¬¦ä¸²
    """
    return LLMConfig.get_base_url()


# ============ æµ‹è¯•ä»£ç  ============

if __name__ == '__main__':
    print("="*60)
    print("LLM é…ç½®ä¸­å¿ƒæµ‹è¯•")
    print("="*60)
    
    # æµ‹è¯• API Key
    api_key = LLMConfig.get_api_key()
    if api_key:
        print(f"\nâœ… API Key: {api_key[:20]}...{api_key[-10:]}")
    else:
        print("\nâŒ API Key æœªé…ç½®")
    
    # æµ‹è¯• Base URL
    print(f"ğŸ“ Base URL: {LLMConfig.get_base_url()}")
    
    # æµ‹è¯•é»˜è®¤æ¨¡å‹
    print("\nğŸ“‹ é»˜è®¤æ¨¡å‹é…ç½®:")
    print(f"  - è´¨é‡è¯„ä¼°: {LLMConfig.get_default_model('evaluation')}")
    print(f"  - æµç¨‹åˆ†æ: {LLMConfig.get_default_model('flow_analysis')}")
    print(f"  - é€šç”¨ä»»åŠ¡: {LLMConfig.get_default_model('general')}")
    
    # æµ‹è¯•è¶…æ—¶é…ç½®
    timeout = LLMConfig.get_timeout()
    print(f"\nâ±ï¸  è¶…æ—¶é…ç½®: è¿æ¥ {timeout[0]}s, è¯»å– {timeout[1]}s")
    
    # æµ‹è¯•é‡è¯•é…ç½®
    retry = LLMConfig.get_retry_config()
    print(f"ğŸ”„ é‡è¯•é…ç½®: æ€»æ¬¡æ•° {retry['total']}, é€€é¿å› å­ {retry['backoff']}")
    
    # æµ‹è¯•æ¨¡å‹åˆ—è¡¨
    print(f"\nğŸ¯ æ”¯æŒçš„æ¨¡å‹æ€»æ•°: {len(LLMConfig.list_models())}")
    
    # æŒ‰æˆæœ¬åˆ†ç»„
    print("\nğŸ’° æŒ‰æˆæœ¬åˆ†ç»„:")
    for cost_level, models in LLMConfig.get_models_by_cost().items():
        if models:
            print(f"  {cost_level}: {len(models)} ä¸ªæ¨¡å‹")
    
    # æµ‹è¯•æ¨¡å‹æ¨è
    print("\nğŸŒŸ æ™ºèƒ½æ¨è:")
    print(f"  æˆæœ¬ä¼˜å…ˆ (è¯„ä¼°): {LLMConfig.recommend_model('evaluation', 'cost')}")
    print(f"  é€Ÿåº¦ä¼˜å…ˆ (æµç¨‹): {LLMConfig.recommend_model('flow_analysis', 'speed')}")
    print(f"  è´¨é‡ä¼˜å…ˆ (é€šç”¨): {LLMConfig.recommend_model('general', 'quality')}")
    
    # æµ‹è¯•æ¨¡å‹ä¿¡æ¯
    test_model = "deepseek-ai/DeepSeek-V3.2"
    info = LLMConfig.get_model_info(test_model)
    if info:
        print(f"\nğŸ“– æ¨¡å‹ä¿¡æ¯: {test_model}")
        print(f"  åç§°: {info['name']}")
        print(f"  æä¾›å•†: {info['provider']}")
        print(f"  æˆæœ¬: {info['cost']} | é€Ÿåº¦: {info['speed']} | è´¨é‡: {info['quality']}")
        print(f"  æè¿°: {info['description']}")
    
    print("\n" + "="*60)
