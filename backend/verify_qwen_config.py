"""éªŒè¯ Qwen2.5 é…ç½®æ›´æ–°"""
from config.llm_config import LLMConfig

print("="*70)
print("Qwen2.5 é…ç½®éªŒè¯")
print("="*70)

# æ£€æŸ¥é»˜è®¤æ¨¡å‹
print("\nğŸ“‹ é»˜è®¤æ¨¡å‹é…ç½®ï¼š")
print(f"   è¯„ä¼°ä»»åŠ¡:    {LLMConfig.get_default_model('evaluation')}")
print(f"   æµç¨‹åˆ†æ:    {LLMConfig.get_default_model('flow_analysis')}")
print(f"   é€šç”¨ä»»åŠ¡:    {LLMConfig.get_default_model('general')}")

# æ£€æŸ¥æ¨¡å‹ä¿¡æ¯
print("\nğŸ“Š Qwen2.5 æ¨¡å‹è¯¦æƒ…ï¼š")
model_info = LLMConfig.get_model_info("Qwen/Qwen2.5-7B-Instruct")
if model_info:
    print(f"   åç§°:        {model_info['name']}")
    print(f"   æä¾›å•†:      {model_info['provider']}")
    print(f"   è´¹ç”¨:        {model_info['cost']}")
    print(f"   é€Ÿåº¦:        {model_info['speed']}")
    print(f"   è´¨é‡:        {model_info['quality']}")
    print(f"   æè¿°:        {model_info['description']}")
    if 'test_results' in model_info:
        print(f"\n   æµ‹è¯•ç»“æœ:")
        print(f"     æˆåŠŸç‡:       {model_info['test_results']['success_rate']}")
        print(f"     å¹³å‡å“åº”:     {model_info['test_results']['avg_response_time']}ç§’")
        print(f"     JSONå…¼å®¹æ€§:   {model_info['test_results']['json_compatibility']}")

# éªŒè¯æ‰€æœ‰ä»»åŠ¡éƒ½ä½¿ç”¨ Qwen2.5
print("\nâœ… é…ç½®çŠ¶æ€æ£€æŸ¥ï¼š")
all_qwen = True
for task in ["evaluation", "flow_analysis", "general"]:
    model = LLMConfig.get_default_model(task)
    is_qwen = model == "Qwen/Qwen2.5-7B-Instruct"
    status = "âœ…" if is_qwen else "âŒ"
    print(f"   {status} {task}: {model}")
    if not is_qwen:
        all_qwen = False

if all_qwen:
    print("\nğŸ‰ é…ç½®å®Œæˆï¼æ‰€æœ‰ä»»åŠ¡å·²ç»Ÿä¸€ä½¿ç”¨ Qwen/Qwen2.5-7B-Instruct")
    print("\nğŸ’¡ ä¸‹ä¸€æ­¥ï¼š")
    print("   1. é‡å¯åç«¯æœåŠ¡ï¼špython backend/start_server.py")
    print("   2. æµ‹è¯•è¯„ä¼°æ¥å£ï¼ŒéªŒè¯ JSON è¾“å‡ºæ­£å¸¸")
else:
    print("\nâš ï¸  è­¦å‘Šï¼šéƒ¨åˆ†ä»»åŠ¡æœªä½¿ç”¨ Qwen2.5ï¼Œè¯·æ£€æŸ¥é…ç½®")

print("="*70)
