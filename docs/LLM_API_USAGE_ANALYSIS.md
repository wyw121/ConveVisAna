# ConveVisAna åç«¯ LLM API ä½¿ç”¨æƒ…å†µåˆ†æ

> ğŸ“… ç”Ÿæˆæ—¥æœŸ: 2026å¹´1æœˆ2æ—¥  
> ğŸ¯ ç›®çš„: å…¨é¢æ¢³ç†é¡¹ç›®ä¸­ LLM API çš„ä½¿ç”¨æƒ…å†µï¼Œæå‡ºç»Ÿä¸€ç®¡ç†æ–¹æ¡ˆ

---

## ğŸ“‹ ç›®å½•

- [å½“å‰ä½¿ç”¨æ¦‚å†µ](#å½“å‰ä½¿ç”¨æ¦‚å†µ)
- [API Key é…ç½®åˆ†æ](#api-key-é…ç½®åˆ†æ)
- [æ¨¡å‹ä½¿ç”¨ä½ç½®](#æ¨¡å‹ä½¿ç”¨ä½ç½®)
- [å­˜åœ¨çš„é—®é¢˜](#å­˜åœ¨çš„é—®é¢˜)
- [æ”¹è¿›æ–¹æ¡ˆ](#æ”¹è¿›æ–¹æ¡ˆ)
- [å®æ–½å»ºè®®](#å®æ–½å»ºè®®)

---

## ğŸ” å½“å‰ä½¿ç”¨æ¦‚å†µ

### æ”¯æŒçš„ API æœåŠ¡

é¡¹ç›®å½“å‰æ”¯æŒä»¥ä¸‹ LLM API æœåŠ¡ï¼š

| æœåŠ¡ç±»å‹ | ä¼˜å…ˆçº§ | é…ç½®æ–¹å¼ | çŠ¶æ€ |
|---------|--------|---------|------|
| **ChatAIAPI** (è½¬å‘æœåŠ¡) | ğŸ¥‡ é¦–é€‰ | `CHATAIAPI_KEY` / `CHATAI_API_KEY` | âœ… æ­£å¸¸ä½¿ç”¨ |
| **OpenAI API** (åŸç”Ÿ) | ğŸ¥ˆ å¤‡é€‰ | `OPENAI_API_KEY` | âš ï¸ å¯é€‰é…ç½® |
| **API_KEY_OVERRIDE** | ğŸš€ ä¸´æ—¶ | PowerShell ç¯å¢ƒå˜é‡æ³¨å…¥ | âœ… æ”¯æŒè¿è¡Œæ—¶è¦†ç›– |

### å½“å‰ä½¿ç”¨çš„æ¨¡å‹

| æ¨¡å‹åç§° | ç”¨é€” | ä½ç½® | é»˜è®¤å€¼ |
|---------|------|------|--------|
| `claude-3-5-sonnet-20240620` | è´¨é‡è¯„ä¼° | `evaluate_chats.py` | âœ… æ˜¯ |
| `claude-3-5-sonnet-20240620` | API ç«¯ç‚¹é»˜è®¤ | `main.py` | âœ… æ˜¯ |
| `deepseek-chat` | æµç¨‹åˆ†æ | `conversation_flow_analyzer.py` | âš ï¸ ç¡¬ç¼–ç è°ƒç”¨ |
| ç”¨æˆ·å¯é€‰ | API è¯·æ±‚å‚æ•° | å‰ç«¯ä¼ é€’ | â­ æ”¯æŒåŠ¨æ€åˆ‡æ¢ |

### å…³é”®ç»Ÿè®¡æ•°æ®

- **API Key è¯»å–ä½ç½®**: 5 å¤„
- **æ¨¡å‹é…ç½®ä½ç½®**: 4 å¤„
- **ç¡¬ç¼–ç é»˜è®¤æ¨¡å‹**: 3 å¤„
- **ç¯å¢ƒå˜é‡æ•°é‡**: 7 ä¸ª

---

## ğŸ”‘ API Key é…ç½®åˆ†æ

### ç¯å¢ƒå˜é‡å±‚çº§

é¡¹ç›®ä½¿ç”¨**å¤šå±‚çº§å›é€€æœºåˆ¶**è¯»å– API Keyï¼š

```python
# ä¼˜å…ˆçº§ä»é«˜åˆ°ä½
api_key = (
    os.getenv("API_KEY_OVERRIDE")     # 1. è¿è¡Œæ—¶æ³¨å…¥ï¼ˆæœ€é«˜ä¼˜å…ˆçº§ï¼‰
    or os.getenv("OPENAI_API_KEY")    # 2. OpenAI åŸç”Ÿ API
    or os.getenv("CHATAIAPI_KEY")     # 3. ChatAI APIï¼ˆé¦–é€‰ï¼‰
    or os.getenv("CHATAI_API_KEY")    # 4. ChatAI APIï¼ˆåˆ«åï¼‰
)
```

### é…ç½®æ–‡ä»¶ä½ç½®

#### 1. backend/.env ï¼ˆä¸»é…ç½®æ–‡ä»¶ï¼‰

```env
# ChatAIAPI Key - è½¬å‘æœåŠ¡ï¼ˆæ¨èï¼‰
CHATAIAPI_KEY=sk-yEI0NzjjM24Ec0RUpMyrQQGYN6cvGx27F6XCNYEIxyaigQJP
CHATAI_API_KEY=sk-yEI0NzjjM24Ec0RUpMyrQQGYN6cvGx27F6XCNYEIxyaigQJP

# API åŸºç¡€ URL
CHATAI_BASE_URL=https://api.chataiapi.com/v1

# å¯é€‰ï¼šOpenAI åŸç”Ÿ API
# OPENAI_API_KEY=your_openai_api_key_here
```

#### 2. backend/.env.example ï¼ˆé…ç½®æ¨¡æ¿ï¼‰

æä¾›äº†å®Œæ•´çš„é…ç½®è¯´æ˜å’Œç¤ºä¾‹å€¼ã€‚

### API Key è¯»å–ä½ç½®æ±‡æ€»

| æ–‡ä»¶è·¯å¾„ | è¯»å–æ–¹å¼ | ç”¨é€” |
|---------|---------|------|
| `api/main.py` (L106-110) | å¤šå±‚çº§å›é€€ | `/api/health` å¥åº·æ£€æŸ¥ |
| `api/main.py` (L141-145) | å¤šå±‚çº§å›é€€ | `/api/evaluate-quality` è¯„ä¼°ç«¯ç‚¹ |
| `api/main.py` (L251-255) | å¤šå±‚çº§å›é€€ | `/api/analyze-flow` æµç¨‹åˆ†æ |
| `core/evaluate_chats.py` (L64-67) | ä¸‰å±‚å›é€€ | è¯„ä¼°å™¨åˆå§‹åŒ– |
| `core/custom_llm.py` | å‚æ•°ä¼ é€’ | LLM æ¨¡å‹åˆå§‹åŒ– |

### âš ï¸ å‘ç°çš„é—®é¢˜

1. **ä»£ç é‡å¤**: API Key è¯»å–é€»è¾‘åœ¨ 3 ä¸ªä¸åŒæ–‡ä»¶ä¸­é‡å¤å‡ºç°
2. **ä¸ä¸€è‡´æ€§**: æŸäº›åœ°æ–¹ç¼ºå°‘ `API_KEY_OVERRIDE` æ”¯æŒ
3. **ç»´æŠ¤å›°éš¾**: ä¿®æ”¹è¯»å–é€»è¾‘éœ€è¦åŒæ­¥å¤šå¤„ä»£ç 

---

## ğŸ“ æ¨¡å‹ä½¿ç”¨ä½ç½®

### 1. backend/core/custom_llm.py

**æ ¸å¿ƒ LLM é€‚é…å™¨** - æ‰€æœ‰æ¨¡å‹è°ƒç”¨çš„åŸºç¡€ç±»

```python
class ChatAIAPIModel(DeepEvalBaseLLM):
    def __init__(
        self,
        api_key: str,
        model: str = "claude-3-5-sonnet-20240620",  # âš ï¸ ç¡¬ç¼–ç é»˜è®¤å€¼
        base_url: str = "https://api.chataiapi.com/v1"
    ):
        # ... æ¨¡å‹åˆå§‹åŒ–é€»è¾‘
```

**ä¾¿æ·å‡½æ•°**:
```python
def create_claude_sonnet(api_key: str) -> ChatAIAPIModel:
    return ChatAIAPIModel(api_key=api_key, model="claude-3-5-sonnet-20240620")

def create_deepseek_chat(api_key: str) -> ChatAIAPIModel:
    return ChatAIAPIModel(api_key=api_key, model="deepseek-chat")
```

**é…ç½®é¡¹**:
- âœ… `CHATAI_BASE_URL` - API åŸºç¡€åœ°å€
- âœ… `CHATAI_TIMEOUT` - è¯·æ±‚è¶…æ—¶ (æ ¼å¼: "è¿æ¥,è¯»å–" æˆ–å•å€¼)
- âœ… `CHATAI_RETRY_TOTAL` - é‡è¯•æ¬¡æ•° (é»˜è®¤ 3)
- âœ… `CHATAI_RETRY_BACKOFF` - é‡è¯•é€€é¿å› å­ (é»˜è®¤ 1.5)

---

### 2. backend/core/evaluate_chats.py

**è´¨é‡è¯„ä¼°å™¨** - è´Ÿè´£å¯¹è¯è´¨é‡è¯„ä¼°

```python
class ChatQualityEvaluator:
    def __init__(
        self,
        data_folder: str,
        model: str = "claude-3-5-sonnet-20240620",  # âš ï¸ ç¡¬ç¼–ç é»˜è®¤å€¼
        use_custom_api: bool = True
    ):
        # ... åˆå§‹åŒ–è¯„ä¼°æŒ‡æ ‡
```

**ä½¿ç”¨çš„è¯„ä¼°æŒ‡æ ‡**:
- `relevancy` - ç­”æ¡ˆç›¸å…³æ€§
- `helpfulness` - æœ‰ç”¨æ€§ (GEval)
- `coherence` - è¿è´¯æ€§ (GEval)
- `empathy` - å…±æƒ…èƒ½åŠ› (GEval)
- `toxicity` - æ¯’æ€§æ£€æµ‹
- `bias` - åè§æ£€æµ‹

**ç‰¹ç‚¹**:
- æ‰€æœ‰æŒ‡æ ‡éƒ½ä½¿ç”¨åŒä¸€ä¸ª LLM æ¨¡å‹
- æ”¯æŒè‡ªå®šä¹‰æ¨¡å‹å’ŒåŸç”Ÿ OpenAI API

---

### 3. backend/core/conversation_flow_analyzer.py

**æµç¨‹åˆ†æå™¨** - åˆ†æå¯¹è¯å‘å±•è¿‡ç¨‹

```python
class ConversationFlowAnalyzer:
    def __init__(self, model):
        self.model = model  # æ¥æ”¶å¤–éƒ¨ä¼ å…¥çš„æ¨¡å‹å®ä¾‹
```

**è°ƒç”¨ä½ç½®** (`api/main.py` L279):
```python
# åˆ›å»º LLM æ¨¡å‹
model = create_deepseek_chat(api_key)  # âš ï¸ ç¡¬ç¼–ç ä½¿ç”¨ DeepSeek

# åˆ›å»ºåˆ†æå™¨
analyzer = ConversationFlowAnalyzer(model)
```

**åˆ†æåŠŸèƒ½**:
- é—®é¢˜åˆ†ç±» (clarifying, deepening, emotional, technical, off-topic)
- ä»·å€¼ç­‰çº§è¯„ä¼° (high, medium, low)
- è¯é¢˜è½¬ç§»æ£€æµ‹
- æµç¨‹æ‘˜è¦ç”Ÿæˆ

---

### 4. backend/api/main.py

**FastAPI ç«¯ç‚¹** - æ¥æ”¶å‰ç«¯è¯·æ±‚

#### ç«¯ç‚¹ 1: `/api/evaluate-quality` (L123)

```python
async def evaluate_quality(
    file: UploadFile = File(...),
    max_qa_pairs: int = 3,
    model: str = "claude-3-5-sonnet-20240620"  # âš ï¸ ç¡¬ç¼–ç é»˜è®¤å€¼
):
    # åˆ›å»ºè¯„ä¼°å™¨
    evaluator = ChatQualityEvaluator(
        str(data_folder),
        model=model,  # âœ… æ”¯æŒåŠ¨æ€ä¼ å…¥
        use_custom_api=True
    )
```

#### ç«¯ç‚¹ 2: `/api/analyze-flow` (L237)

```python
async def analyze_flow(file: UploadFile = File(...)):
    # åˆ›å»º LLM æ¨¡å‹
    model = create_deepseek_chat(api_key)  # âš ï¸ å›ºå®šä½¿ç”¨ DeepSeek
    
    # åˆ›å»ºåˆ†æå™¨
    analyzer = ConversationFlowAnalyzer(model)
```

### æ¨¡å‹ä½¿ç”¨çŸ©é˜µ

| åŠŸèƒ½æ¨¡å— | é»˜è®¤æ¨¡å‹ | æ˜¯å¦å¯é…ç½® | é…ç½®æ–¹å¼ |
|---------|---------|-----------|---------|
| è´¨é‡è¯„ä¼° API | Claude Sonnet 3.5 | âœ… æ˜¯ | API è¯·æ±‚å‚æ•° `model` |
| æµç¨‹åˆ†æ API | DeepSeek Chat | âŒ å¦ | ç¡¬ç¼–ç  |
| è¯„ä¼°å™¨ç±» | Claude Sonnet 3.5 | âœ… æ˜¯ | æ„é€ å‡½æ•°å‚æ•° |
| LLM é€‚é…å™¨ | Claude Sonnet 3.5 | âœ… æ˜¯ | æ„é€ å‡½æ•°å‚æ•° |

---

## âš ï¸ å­˜åœ¨çš„é—®é¢˜

### é—®é¢˜ 1: é…ç½®åˆ†æ•£ ğŸ”´ ä¸¥é‡

**ç°è±¡**: æ¨¡å‹åç§°ç¡¬ç¼–ç åœ¨å¤šä¸ªæ–‡ä»¶ä¸­

```python
# custom_llm.py
model: str = "claude-3-5-sonnet-20240620"

# evaluate_chats.py
model: str = "claude-3-5-sonnet-20240620"

# main.py
model: str = "claude-3-5-sonnet-20240620"
```

**å½±å“**:
- æ›´æ¢é»˜è®¤æ¨¡å‹éœ€è¦ä¿®æ”¹ 3+ å¤„ä»£ç 
- å®¹æ˜“é—æ¼æŸä¸ªä½ç½®å¯¼è‡´ä¸ä¸€è‡´
- æ— æ³•å…¨å±€å¿«é€Ÿåˆ‡æ¢æ¨¡å‹

---

### é—®é¢˜ 2: API Key è¯»å–é€»è¾‘é‡å¤ ğŸŸ¡ ä¸­ç­‰

**ç°è±¡**: åŒæ ·çš„ä»£ç å‡ºç°åœ¨å¤šä¸ªæ–‡ä»¶

```python
# api/main.py - å‡ºç° 3 æ¬¡
api_key = (
    os.getenv("API_KEY_OVERRIDE")
    or os.getenv("OPENAI_API_KEY")
    or os.getenv("CHATAIAPI_KEY")
    or os.getenv("CHATAI_API_KEY")
)

# core/evaluate_chats.py - ç•¥æœ‰å·®å¼‚
api_key = (
    os.getenv('API_KEY_OVERRIDE')
    or os.getenv('CHATAIAPI_KEY')
    or os.getenv('CHATAI_API_KEY')
)
```

**å½±å“**:
- DRY åŸåˆ™è¿èƒŒ
- ä¿®æ”¹ä¼˜å…ˆçº§éœ€è¦åŒæ­¥å¤šå¤„
- æŸäº›åœ°æ–¹ä¸æ”¯æŒ `OPENAI_API_KEY`

---

### é—®é¢˜ 3: æµç¨‹åˆ†ææ¨¡å‹å›ºå®š ğŸŸ¡ ä¸­ç­‰

**ç°è±¡**: `analyze_flow` ç«¯ç‚¹ç¡¬ç¼–ç ä½¿ç”¨ DeepSeek

```python
# api/main.py L279
model = create_deepseek_chat(api_key)  # æ— æ³•ä¿®æ”¹
```

**å½±å“**:
- ç”¨æˆ·æ— æ³•ä¸ºæµç¨‹åˆ†æé€‰æ‹©å…¶ä»–æ¨¡å‹
- ä¸è´¨é‡è¯„ä¼° API çš„çµæ´»æ€§ä¸ä¸€è‡´
- å¦‚æœ DeepSeek ä¸å¯ç”¨ä¼šå¯¼è‡´åŠŸèƒ½å¤±æ•ˆ

---

### é—®é¢˜ 4: ç¼ºå°‘é›†ä¸­é…ç½®æ–‡ä»¶ ğŸŸ¢ è½»å¾®

**ç°è±¡**: æ²¡æœ‰å•ä¸€çš„é…ç½®ç®¡ç†æ¨¡å—

**å½±å“**:
- æ–°å¢æ¨¡å‹æ”¯æŒéœ€è¦ä¿®æ”¹å¤šå¤„
- é…ç½®é¡¹æ–‡æ¡£åˆ†æ•£
- éš¾ä»¥å®ç°é…ç½®çƒ­é‡è½½

---

### é—®é¢˜ 5: ç¯å¢ƒå˜é‡å‘½åä¸ç»Ÿä¸€ ğŸŸ¢ è½»å¾®

**ç°è±¡**: åŒä¸€é…ç½®æœ‰å¤šä¸ªåˆ«å

```env
CHATAIAPI_KEY=xxx
CHATAI_API_KEY=xxx
OPENAI_API_KEY=xxx
API_KEY_OVERRIDE=xxx
```

**å½±å“**:
- ç”¨æˆ·å®¹æ˜“æ··æ·†ä½¿ç”¨å“ªä¸ªå˜é‡å
- æ–‡æ¡£éœ€è¦è§£é‡Šå¤šä¸ªåç§°
- å¢åŠ ç†è§£æˆæœ¬

---

## ğŸ’¡ æ”¹è¿›æ–¹æ¡ˆ

### æ–¹æ¡ˆæ¦‚è¿°

åˆ›å»º**ç»Ÿä¸€é…ç½®ç®¡ç†ä¸­å¿ƒ**ï¼Œé›†ä¸­ç®¡ç†æ‰€æœ‰ LLM ç›¸å…³é…ç½®ã€‚

### æ¶æ„è®¾è®¡

```
backend/
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ llm_config.py      # ğŸ†• LLM é…ç½®ç®¡ç†ä¸­å¿ƒ
â”‚   â””â”€â”€ models.py          # ğŸ†• æ¨¡å‹é¢„è®¾é…ç½®
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ custom_llm.py      # âœï¸ ç®€åŒ–ï¼Œä½¿ç”¨é…ç½®
â”‚   â”œâ”€â”€ evaluate_chats.py  # âœï¸ ä½¿ç”¨é…ç½®
â”‚   â””â”€â”€ conversation_flow_analyzer.py
â””â”€â”€ api/
    â””â”€â”€ main.py            # âœï¸ ä½¿ç”¨é…ç½®
```

---

### æ–¹æ¡ˆ 1: åˆ›å»ºé…ç½®ç®¡ç†æ¨¡å— â­ æ¨è

#### æ–‡ä»¶: `backend/config/llm_config.py`

```python
"""
LLM é…ç½®ç®¡ç†ä¸­å¿ƒ
ç»Ÿä¸€ç®¡ç†æ‰€æœ‰ LLM API ç›¸å…³é…ç½®
"""
import os
from typing import Optional, Dict
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv(override=True)


class LLMConfig:
    """LLM é…ç½®ç®¡ç†ç±»"""
    
    # ============ API Key é…ç½® ============
    
    @staticmethod
    def get_api_key() -> Optional[str]:
        """
        è·å– API Keyï¼ˆæŒ‰ä¼˜å…ˆçº§ï¼‰
        
        ä¼˜å…ˆçº§:
        1. API_KEY_OVERRIDE (è¿è¡Œæ—¶æ³¨å…¥)
        2. CHATAIAPI_KEY (ChatAI ä¸»é”®)
        3. CHATAI_API_KEY (ChatAI åˆ«å)
        4. OPENAI_API_KEY (OpenAI åŸç”Ÿ)
        """
        return (
            os.getenv("API_KEY_OVERRIDE")
            or os.getenv("CHATAIAPI_KEY")
            or os.getenv("CHATAI_API_KEY")
            or os.getenv("OPENAI_API_KEY")
        )
    
    @staticmethod
    def get_base_url() -> str:
        """è·å– API åŸºç¡€ URL"""
        return os.getenv(
            "CHATAI_BASE_URL", 
            "https://api.chataiapi.com/v1"
        )
    
    # ============ æ¨¡å‹é…ç½® ============
    
    # é»˜è®¤æ¨¡å‹é…ç½®
    DEFAULT_MODELS = {
        "evaluation": "claude-3-5-sonnet-20240620",  # è´¨é‡è¯„ä¼°é»˜è®¤
        "flow_analysis": "deepseek-chat",            # æµç¨‹åˆ†æé»˜è®¤
        "general": "claude-3-5-sonnet-20240620"      # é€šç”¨é»˜è®¤
    }
    
    @classmethod
    def get_default_model(cls, task: str = "general") -> str:
        """
        è·å–æŒ‡å®šä»»åŠ¡çš„é»˜è®¤æ¨¡å‹
        
        Args:
            task: ä»»åŠ¡ç±»å‹ (evaluation, flow_analysis, general)
        """
        # æ”¯æŒç¯å¢ƒå˜é‡è¦†ç›–
        env_key = f"DEFAULT_MODEL_{task.upper()}"
        return os.getenv(env_key, cls.DEFAULT_MODELS.get(task, cls.DEFAULT_MODELS["general"]))
    
    # ============ è¶…æ—¶å’Œé‡è¯•é…ç½® ============
    
    @staticmethod
    def get_timeout() -> tuple:
        """è·å–è¶…æ—¶é…ç½® (è¿æ¥è¶…æ—¶, è¯»å–è¶…æ—¶)"""
        env_timeout = os.getenv("CHATAI_TIMEOUT")
        if env_timeout:
            try:
                parts = [p.strip() for p in env_timeout.split(',')]
                if len(parts) == 2:
                    return (float(parts[0]), float(parts[1]))
                else:
                    t = float(parts[0])
                    return (t, t)
            except:
                pass
        return (15, 60)  # é»˜è®¤å€¼
    
    @staticmethod
    def get_retry_config() -> Dict[str, int]:
        """è·å–é‡è¯•é…ç½®"""
        return {
            "total": int(os.getenv("CHATAI_RETRY_TOTAL", "3")),
            "backoff": float(os.getenv("CHATAI_RETRY_BACKOFF", "1.5"))
        }
    
    # ============ æ¨¡å‹é¢„è®¾ ============
    
    SUPPORTED_MODELS = {
        # Claude ç³»åˆ—
        "claude-3-5-sonnet-20240620": {
            "name": "Claude 3.5 Sonnet",
            "provider": "Anthropic",
            "cost": "medium",
            "speed": "fast",
            "quality": "excellent",
            "recommended_for": ["evaluation", "complex_reasoning"]
        },
        
        # DeepSeek ç³»åˆ—
        "deepseek-chat": {
            "name": "DeepSeek Chat",
            "provider": "DeepSeek",
            "cost": "low",
            "speed": "very_fast",
            "quality": "good",
            "recommended_for": ["flow_analysis", "batch_processing"]
        },
        "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B": {
            "name": "DeepSeek R1 Distill",
            "provider": "DeepSeek",
            "cost": "very_low",
            "speed": "very_fast",
            "quality": "good",
            "recommended_for": ["reasoning", "fast_response"]
        },
        "deepseek-ai/DeepSeek-V3.2": {
            "name": "DeepSeek V3.2",
            "provider": "DeepSeek",
            "cost": "low",
            "speed": "fast",
            "quality": "excellent",
            "recommended_for": ["comprehensive_analysis"]
        },
        
        # OpenAI ç³»åˆ— (å¦‚æœä½¿ç”¨)
        "gpt-4o-mini": {
            "name": "GPT-4o Mini",
            "provider": "OpenAI",
            "cost": "medium",
            "speed": "fast",
            "quality": "excellent",
            "recommended_for": ["general"]
        }
    }
    
    @classmethod
    def get_model_info(cls, model_name: str) -> Optional[Dict]:
        """è·å–æ¨¡å‹è¯¦ç»†ä¿¡æ¯"""
        return cls.SUPPORTED_MODELS.get(model_name)
    
    @classmethod
    def list_models(cls) -> list:
        """åˆ—å‡ºæ‰€æœ‰æ”¯æŒçš„æ¨¡å‹"""
        return list(cls.SUPPORTED_MODELS.keys())


# ============ ä¾¿æ·å‡½æ•° ============

def get_api_key() -> Optional[str]:
    """å¿«æ·è·å– API Key"""
    return LLMConfig.get_api_key()


def get_model_for_task(task: str) -> str:
    """å¿«æ·è·å–ä»»åŠ¡é»˜è®¤æ¨¡å‹"""
    return LLMConfig.get_default_model(task)
```

**ä¼˜ç‚¹**:
- âœ… å•ä¸€æ•°æ®æº (Single Source of Truth)
- âœ… æ˜“äºç»´æŠ¤å’Œæ‰©å±•
- âœ… æ”¯æŒç¯å¢ƒå˜é‡è¦†ç›–
- âœ… åŒ…å«æ¨¡å‹å…ƒæ•°æ®ä¾¿äºé€‰æ‹©

---

### æ–¹æ¡ˆ 2: ä¿®æ”¹ç°æœ‰ä»£ç ä½¿ç”¨é…ç½®

#### ä¿®æ”¹ `custom_llm.py`

```python
from config.llm_config import LLMConfig

class ChatAIAPIModel(DeepEvalBaseLLM):
    def __init__(
        self,
        api_key: str = None,
        model: str = None,  # æ”¹ä¸ºå¯é€‰
        base_url: str = None
    ):
        # ä½¿ç”¨é…ç½®ä¸­å¿ƒçš„é»˜è®¤å€¼
        self.api_key = api_key or LLMConfig.get_api_key()
        self._requested_model = model or LLMConfig.get_default_model("general")
        self.base_url = (base_url or LLMConfig.get_base_url()).rstrip('/')
        
        # ä½¿ç”¨é…ç½®çš„è¶…æ—¶å’Œé‡è¯•
        self._timeout = LLMConfig.get_timeout()
        retry_config = LLMConfig.get_retry_config()
        # ... å…¶ä½™ä»£ç 
```

#### ä¿®æ”¹ `evaluate_chats.py`

```python
from config.llm_config import get_api_key, get_model_for_task

class ChatQualityEvaluator:
    def __init__(
        self,
        data_folder: str,
        model: str = None,  # æ”¹ä¸ºå¯é€‰
        use_custom_api: bool = True
    ):
        self.data_folder = data_folder
        # ä½¿ç”¨é…ç½®çš„é»˜è®¤æ¨¡å‹
        self.model_name = model or get_model_for_task("evaluation")
        self.use_custom_api = use_custom_api
        
        if use_custom_api:
            api_key = get_api_key()  # ä½¿ç”¨ç»Ÿä¸€å‡½æ•°
            if not api_key:
                raise ValueError("æœªé…ç½® API Key")
            self.custom_llm = ChatAIAPIModel(api_key=api_key, model=self.model_name)
```

#### ä¿®æ”¹ `main.py`

```python
from config.llm_config import get_api_key, get_model_for_task

@app.post("/api/evaluate-quality")
async def evaluate_quality(
    file: UploadFile = File(...),
    max_qa_pairs: int = 3,
    model: str = None  # æ”¹ä¸ºå¯é€‰ï¼Œä½¿ç”¨ None ä½œä¸ºé»˜è®¤å€¼
):
    try:
        api_key = get_api_key()  # ä½¿ç”¨ç»Ÿä¸€å‡½æ•°
        if not api_key:
            raise HTTPException(status_code=500, detail="æœªé…ç½® API Key")
        
        # ä½¿ç”¨é…ç½®çš„é»˜è®¤æ¨¡å‹
        model = model or get_model_for_task("evaluation")
        
        evaluator = ChatQualityEvaluator(
            str(data_folder),
            model=model,
            use_custom_api=True
        )
        # ...

@app.post("/api/analyze-flow")
async def analyze_flow(
    file: UploadFile = File(...),
    model: str = None  # ğŸ†• æ–°å¢æ¨¡å‹å‚æ•°
):
    try:
        api_key = get_api_key()
        if not api_key:
            raise HTTPException(status_code=500, detail="æœªé…ç½® API Key")
        
        # æ”¯æŒåŠ¨æ€é€‰æ‹©æ¨¡å‹
        model_name = model or get_model_for_task("flow_analysis")
        llm_model = ChatAIAPIModel(api_key=api_key, model=model_name)
        
        analyzer = ConversationFlowAnalyzer(llm_model)
        # ...
```

---

### æ–¹æ¡ˆ 3: æ–°å¢ç¯å¢ƒå˜é‡é…ç½®

åœ¨ `.env` ä¸­æ–°å¢ï¼š

```env
# ============ æ¨¡å‹é»˜è®¤é…ç½® ============
# å¯ä»¥é€šè¿‡ç¯å¢ƒå˜é‡è¦†ç›–ä»£ç ä¸­çš„é»˜è®¤å€¼

# è´¨é‡è¯„ä¼°é»˜è®¤æ¨¡å‹
DEFAULT_MODEL_EVALUATION=claude-3-5-sonnet-20240620

# æµç¨‹åˆ†æé»˜è®¤æ¨¡å‹
DEFAULT_MODEL_FLOW_ANALYSIS=deepseek-chat

# é€šç”¨é»˜è®¤æ¨¡å‹
DEFAULT_MODEL_GENERAL=claude-3-5-sonnet-20240620
```

**ä½¿ç”¨åœºæ™¯**:
```bash
# å¿«é€Ÿåˆ‡æ¢è¯„ä¼°æ¨¡å‹ä¸º DeepSeek V3.2
DEFAULT_MODEL_EVALUATION=deepseek-ai/DeepSeek-V3.2 python start_server.py

# æˆ–åœ¨ PowerShell ä¸­
$env:DEFAULT_MODEL_EVALUATION="deepseek-ai/DeepSeek-V3.2"
python start_server.py
```

---

## ğŸš€ å®æ–½å»ºè®®

### é˜¶æ®µ 1: åˆ›å»ºé…ç½®æ¨¡å— (1-2å°æ—¶)

**æ­¥éª¤**:
1. åˆ›å»º `backend/config/` ç›®å½•
2. åˆ›å»º `llm_config.py` æ–‡ä»¶
3. å®ç° `LLMConfig` ç±»
4. æ·»åŠ å•å…ƒæµ‹è¯•

**éªŒè¯**:
```python
# æµ‹è¯•é…ç½®è¯»å–
from config.llm_config import LLMConfig

print("API Key:", LLMConfig.get_api_key()[:20] + "...")
print("è¯„ä¼°æ¨¡å‹:", LLMConfig.get_default_model("evaluation"))
print("æµç¨‹æ¨¡å‹:", LLMConfig.get_default_model("flow_analysis"))
print("æ”¯æŒçš„æ¨¡å‹:", LLMConfig.list_models())
```

---

### é˜¶æ®µ 2: é‡æ„ç°æœ‰ä»£ç  (2-3å°æ—¶)

**ä¼˜å…ˆçº§**:
1. âœ… ä¿®æ”¹ `custom_llm.py` - ä½¿ç”¨é…ç½®è·å–é»˜è®¤å€¼
2. âœ… ä¿®æ”¹ `evaluate_chats.py` - ä½¿ç”¨ç»Ÿä¸€ API Key å‡½æ•°
3. âœ… ä¿®æ”¹ `main.py` - æ”¯æŒæµç¨‹åˆ†ææ¨¡å‹é€‰æ‹©
4. âœ… æ›´æ–° `.env.example` - æ·»åŠ æ–°é…ç½®é¡¹

**æ³¨æ„äº‹é¡¹**:
- ä¿æŒå‘åå…¼å®¹ï¼Œé»˜è®¤å‚æ•°ä½¿ç”¨ `None`
- æ·»åŠ å……åˆ†çš„æ³¨é‡Šè¯´æ˜
- ç¡®ä¿æ‰€æœ‰æµ‹è¯•é€šè¿‡

---

### é˜¶æ®µ 3: æµ‹è¯•å’Œæ–‡æ¡£ (1å°æ—¶)

**æµ‹è¯•æ¸…å•**:
- [ ] API Key è¯»å–æ­£ç¡®
- [ ] é»˜è®¤æ¨¡å‹é…ç½®ç”Ÿæ•ˆ
- [ ] ç¯å¢ƒå˜é‡è¦†ç›–å·¥ä½œ
- [ ] è´¨é‡è¯„ä¼° API æ­£å¸¸
- [ ] æµç¨‹åˆ†æ API æ­£å¸¸
- [ ] æ¨¡å‹åˆ‡æ¢åŠŸèƒ½æ­£å¸¸

**æ–‡æ¡£æ›´æ–°**:
- [ ] æ›´æ–° `README.md` - æ–°å¢é…ç½®è¯´æ˜
- [ ] æ›´æ–° `API_MODEL_GUIDE.md` - ç®€åŒ–æ¨¡å‹åˆ‡æ¢æŒ‡å—
- [ ] åˆ›å»º `CONFIG_GUIDE.md` - è¯¦ç»†é…ç½®è¯´æ˜

---

### é˜¶æ®µ 4: å¢å¼ºåŠŸèƒ½ (å¯é€‰)

#### åŠŸèƒ½ 1: æ¨¡å‹æ€§èƒ½ç›‘æ§

```python
class LLMConfig:
    @staticmethod
    def log_model_usage(model: str, task: str, tokens: int, duration: float):
        """è®°å½•æ¨¡å‹ä½¿ç”¨æƒ…å†µ"""
        # å¯ä»¥è®°å½•åˆ°æ—¥å¿—æˆ–æ•°æ®åº“
        pass
```

#### åŠŸèƒ½ 2: æˆæœ¬ä¼°ç®—

```python
class LLMConfig:
    PRICING = {
        "claude-3-5-sonnet-20240620": {"input": 0.003, "output": 0.015},
        "deepseek-chat": {"input": 0.0001, "output": 0.0002},
        # ...
    }
    
    @classmethod
    def estimate_cost(cls, model: str, input_tokens: int, output_tokens: int) -> float:
        """ä¼°ç®—æˆæœ¬ (USD)"""
        pricing = cls.PRICING.get(model)
        if not pricing:
            return 0.0
        return (input_tokens * pricing["input"] / 1000 + 
                output_tokens * pricing["output"] / 1000)
```

#### åŠŸèƒ½ 3: æ¨¡å‹å¥åº·æ£€æŸ¥

```python
from config.llm_config import LLMConfig

async def check_model_availability(model: str) -> bool:
    """æ£€æŸ¥æ¨¡å‹æ˜¯å¦å¯ç”¨"""
    try:
        api_key = LLMConfig.get_api_key()
        test_model = ChatAIAPIModel(api_key=api_key, model=model)
        test_model.generate("test")
        return True
    except:
        return False

# åœ¨å¯åŠ¨æ—¶æ£€æŸ¥
@app.on_event("startup")
async def startup_check():
    default_model = LLMConfig.get_default_model("evaluation")
    is_available = await check_model_availability(default_model)
    if not is_available:
        logger.warning(f"é»˜è®¤æ¨¡å‹ {default_model} ä¸å¯ç”¨")
```

---

## ğŸ“Š å¯¹æ¯”ï¼šæ”¹è¿›å‰å

### åˆ‡æ¢æ¨¡å‹å¯¹æ¯”

#### æ”¹è¿›å‰ âŒ

è¦å°†æ‰€æœ‰æ¨¡å‹ä» Claude åˆ‡æ¢åˆ° DeepSeek V3.2ï¼š

```bash
# éœ€è¦ä¿®æ”¹ 3 ä¸ªæ–‡ä»¶çš„ä»£ç 
1. vim backend/core/custom_llm.py      # ä¿®æ”¹é»˜è®¤å€¼
2. vim backend/core/evaluate_chats.py  # ä¿®æ”¹é»˜è®¤å€¼
3. vim backend/api/main.py             # ä¿®æ”¹é»˜è®¤å€¼
4. é‡å¯æœåŠ¡
```

#### æ”¹è¿›å âœ…

æ–¹æ³• 1 - ä¿®æ”¹é…ç½®æ–‡ä»¶ï¼š
```bash
# ä¿®æ”¹ .env
DEFAULT_MODEL_EVALUATION=deepseek-ai/DeepSeek-V3.2
DEFAULT_MODEL_FLOW_ANALYSIS=deepseek-ai/DeepSeek-V3.2
DEFAULT_MODEL_GENERAL=deepseek-ai/DeepSeek-V3.2
```

æ–¹æ³• 2 - è¿è¡Œæ—¶è¦†ç›–ï¼š
```powershell
$env:DEFAULT_MODEL_EVALUATION="deepseek-ai/DeepSeek-V3.2"
python start_server.py
```

æ–¹æ³• 3 - API è¯·æ±‚æ—¶æŒ‡å®šï¼š
```javascript
// å‰ç«¯ä»£ç 
await fetch('/api/evaluate-quality', {
  method: 'POST',
  body: formData,
  headers: {
    'X-Model': 'deepseek-ai/DeepSeek-V3.2'
  }
})
```

---

### ä»£ç ç»´æŠ¤å¯¹æ¯”

| æ–¹é¢ | æ”¹è¿›å‰ | æ”¹è¿›å |
|------|--------|--------|
| **API Key è¯»å–** | 3+ å¤„é‡å¤ä»£ç  | 1 ä¸ªç»Ÿä¸€å‡½æ•° |
| **é»˜è®¤æ¨¡å‹é…ç½®** | 3 å¤„ç¡¬ç¼–ç  | 1 ä¸ªé…ç½®æ–‡ä»¶ |
| **æ·»åŠ æ–°æ¨¡å‹** | éœ€è¦æ–‡æ¡£è¯´æ˜ | è‡ªåŠ¨åŒ…å«å…ƒæ•°æ® |
| **åˆ‡æ¢æ¨¡å‹** | ä¿®æ”¹ä»£ç  | ä¿®æ”¹é…ç½® |
| **æµ‹è¯•éš¾åº¦** | éœ€è¦ mock ç¯å¢ƒå˜é‡ | ç›´æ¥æ³¨å…¥é…ç½® |

---

## ğŸ¯ å¿«é€Ÿå¼€å§‹ï¼šä½¿ç”¨æ–°é…ç½®ç³»ç»Ÿ

### ç¤ºä¾‹ 1: å¿«é€Ÿåˆ‡æ¢åˆ° SiliconFlow çš„ DeepSeek æ¨¡å‹

```bash
# 1. ä¿®æ”¹ .env
CHATAIAPI_KEY=sk-pgorpekchnscrzsupkywglclsxxouuhzjtiierkcxaoxxxqu
CHATAI_BASE_URL=https://api.siliconflow.cn/v1

# 2. é…ç½®é»˜è®¤æ¨¡å‹
DEFAULT_MODEL_EVALUATION=deepseek-ai/DeepSeek-V3.2
DEFAULT_MODEL_FLOW_ANALYSIS=deepseek-ai/DeepSeek-R1-Distill-Qwen-7B

# 3. å¯åŠ¨æœåŠ¡
python backend/start_server.py
```

### ç¤ºä¾‹ 2: æ··åˆä½¿ç”¨ä¸åŒæ¨¡å‹

```python
# è´¨é‡è¯„ä¼°ç”¨é«˜è´¨é‡æ¨¡å‹
evaluator = ChatQualityEvaluator(
    data_folder,
    model="claude-3-5-sonnet-20240620"  # é«˜è´¨é‡
)

# æµç¨‹åˆ†æç”¨å¿«é€Ÿæ¨¡å‹
analyzer = ConversationFlowAnalyzer(
    ChatAIAPIModel(model="deepseek-ai/DeepSeek-R1-Distill-Qwen-7B")  # å¿«é€Ÿ
)
```

### ç¤ºä¾‹ 3: æˆæœ¬ä¼˜åŒ–é…ç½®

```env
# ä½¿ç”¨æœ€ç»æµçš„æ¨¡å‹ç»„åˆ
DEFAULT_MODEL_EVALUATION=deepseek-chat          # æˆæœ¬ä½
DEFAULT_MODEL_FLOW_ANALYSIS=deepseek-ai/DeepSeek-R1-Distill-Qwen-7B  # æœ€ä¾¿å®œ

# æ‰¹é‡å¤„ç†æ—¶çš„è¶…æ—¶é…ç½®
CHATAI_TIMEOUT=20,120  # è¿æ¥20s, è¯»å–120s
CHATAI_RETRY_TOTAL=5   # å¢åŠ é‡è¯•æ¬¡æ•°
```

---

## ğŸ“ æ€»ç»“

### å½“å‰çŠ¶å†µ

- âŒ é…ç½®åˆ†æ•£åœ¨å¤šä¸ªæ–‡ä»¶
- âŒ API Key è¯»å–é€»è¾‘é‡å¤
- âŒ æ¨¡å‹ç¡¬ç¼–ç ï¼Œéš¾ä»¥åˆ‡æ¢
- âŒ ç¼ºå°‘ç»Ÿä¸€ç®¡ç†æœºåˆ¶

### æ”¹è¿›ç›®æ ‡

- âœ… åˆ›å»ºç»Ÿä¸€é…ç½®ä¸­å¿ƒ
- âœ… æ¶ˆé™¤é‡å¤ä»£ç 
- âœ… æ”¯æŒçµæ´»æ¨¡å‹åˆ‡æ¢
- âœ… æä¾›æ¨¡å‹å…ƒæ•°æ®ç®¡ç†

### é¢„æœŸæ”¶ç›Š

1. **å¼€å‘æ•ˆç‡**: ä¿®æ”¹é…ç½®ä» "3ä¸ªæ–‡ä»¶" â†’ "1ä¸ªæ–‡ä»¶"
2. **å¯ç»´æŠ¤æ€§**: ç»Ÿä¸€æ•°æ®æºï¼Œå‡å°‘ 70% é‡å¤ä»£ç 
3. **çµæ´»æ€§**: æ”¯æŒè¿è¡Œæ—¶ã€ç¯å¢ƒå˜é‡ã€API å‚æ•°ä¸‰ç§é…ç½®æ–¹å¼
4. **å¯æ‰©å±•æ€§**: æ–°å¢æ¨¡å‹åªéœ€æ›´æ–°é…ç½®æ–‡ä»¶

### å»ºè®®ä¼˜å…ˆçº§

1. ğŸ”´ **é«˜ä¼˜å…ˆçº§**: åˆ›å»ºé…ç½®æ¨¡å— (ç«‹å³å®æ–½)
2. ğŸŸ¡ **ä¸­ä¼˜å…ˆçº§**: é‡æ„ç°æœ‰ä»£ç  (æœ¬å‘¨å®Œæˆ)
3. ğŸŸ¢ **ä½ä¼˜å…ˆçº§**: å¢å¼ºåŠŸèƒ½ (åç»­è¿­ä»£)

---

## ğŸ”— ç›¸å…³æ–‡æ¡£

- [API_MODEL_GUIDE.md](API_MODEL_GUIDE.md) - æ¨¡å‹é…ç½®æŒ‡å—
- [README.md](../README.md) - é¡¹ç›®ä¸»æ–‡æ¡£
- [backend/.env.example](../backend/.env.example) - é…ç½®æ¨¡æ¿

---

**æ–‡æ¡£ç”Ÿæˆæ—¶é—´**: 2026å¹´1æœˆ2æ—¥  
**ä½œè€…**: GitHub Copilot  
**ç‰ˆæœ¬**: v1.0

